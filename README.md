# **NYC-TAXI-DE-E2E-Project**

## **Executive Summary**

This project is an end-to-end implementation of a modern data engineering pipeline, showcasing advanced skills in data ingestion, transformation, modeling, and delivery. Using the NYC Taxi dataset, it demonstrates the application of industry best practices, scalable architecture, and cutting-edge technologies. The project follows the Medallion Architecture and integrates tools like Azure, Databricks, and Delta Lake to process and deliver high-quality, analysis-ready data. This work is designed to highlight proficiency in building robust, automated data solutions tailored to real-world business scenarios.

## **Objectives**

**Automate Data Ingestion:** Dynamically fetch data from APIs, eliminating manual intervention.

**Implement Medallion Architecture:** Organize data into Bronze, Silver, and Gold layers for structured processing.

**Optimize Data Processing:** Leverage PySpark and Databricks for efficient large-scale transformations.

**Ensure Data Quality and Reliability:** Utilize Delta Lake features like ACID transactions, data versioning, and time travel.

**Deliver Business Value:** Enable seamless data access for analytics and visualization using Power BI.

## **Tools and Technologies**

**Azure Data Factory:** Orchestrates data pipelines with parameterization and dynamic configurations.

**Azure Data Lake Storage (Gen2):** Provides hierarchical storage for big data, optimized for scalability.

**Databricks:** Facilitates distributed data processing using PySpark.

**Delta Lake:** Enhances data management with features like schema enforcement, versioning, and transactional integrity.

**Parquet File Format:** Stores data in a columnar format, improving query performance and storage efficiency.

**Power BI:** Connects to the Gold layer for real-time reporting and analytics.

**GitHub:** Hosts project documentation, scripts, and datasets for version control.

## **Architecture and Methodology**

**Medallion Architecture**

**Bronze Layer:** Ingests raw data directly from APIs, storing it in Parquet format.

**Silver Layer:** Cleans and standardizes data to ensure consistency and usability.

**Gold Layer:** Models data for specific business use cases, making it ready for analytics.
![e2e de project](https://github.com/sumitrawall/NYC-Taxi-end-to-end-azure-datapipeline/blob/main/Extras/Project.jpg?raw=true)

## **Security and Reliability**

**Data Redundancy:** Configured Local (LRS) redundancy for high availability.

**Managed Identities:** Implemented to secure resource access and automate authentication processes.

## **Implementation Details**

**1. Dynamic Data Ingestion with Azure Data Factory**

Azure Data Factory (ADF) was used as the primary orchestration tool for automating the data ingestion process. Each step of the pipeline is explained in detail below:

Step 1: Create a Linked Service

Step 2: Create Datasets

Step 3: Build a Pipeline

Step 4: Parameterize the Pipeline

Step 5: Test and Debug
![ADF Copy 7](https://github.com/sumitrawall/NYC-Taxi-end-to-end-azure-datapipeline/blob/main/Extras/ADF%20Copy%207.png?raw=true)
![Copy if condition](https://github.com/sumitrawall/NYC-Taxi-end-to-end-azure-datapipeline/blob/main/Extras/Copy%20if%20condition.png?raw=true)


**2. Raw Data Storage (Bronze Layer)**

**Objective:** Efficiently store raw data for processing.

**Approach:** Used Azure Data Lake with hierarchical namespaces enabled for structured storage. Data was stored in Parquet format for performance optimization.

**Outcome:** Enabled organized and efficient storage of high-volume data.

**3. Data Transformation (Silver Layer)**

**Objective:** Standardize and clean data for usability.

**Approach:** Applied PySpark in Databricks to remove null values, enforce schema consistency, and integrate lookup data.

**Outcome:** Generated clean, structured data ready for aggregation and modeling.
![Data analysis in ADB](https://github.com/sumitrawall/NYC-Taxi-end-to-end-azure-datapipeline/blob/main/Extras/Data%20analysis%20in%20ADB.png?raw=true)


**4. Data Modeling and Aggregation (Gold Layer)**

**Objective:** Prepare data for business insights.

**Approach:** Aggregated and modeled data into Delta Lake tables, leveraging features like time travel and ACID compliance.

**Outcome:** Produced analysis-ready data optimized for querying and reporting.
![gold-delta tables](https://github.com/sumitrawall/NYC-Taxi-end-to-end-azure-datapipeline/blob/main/Extras/gold-delta%20tables.png?raw=true)


**5. Data Delivery for Analytics**

**Objective:** Enable data visualization and reporting.

**Approach:** Established a direct connection between Azure Databricks and Power BI. Ensured optimal data access for dashboards and analytics.

**Outcome:** Delivered high-quality data to stakeholders for actionable insights.

## **Key Features and Innovations**

**Dynamic Pipelines:** Automated data ingestion with reusability and scalability.

**Advanced Data Management:** Leveraged Delta Lake for schema evolution, version control, and transaction logs.

**Performance Optimization:** Used Parquet format and distributed computing for efficient processing.

**Real-World Architecture:** Implemented Medallion Architecture to mirror industry standards.

**Secure and Reliable:** Ensured data security through managed identities and redundancy configurations.

## **Business Impact**

**Efficiency Gains:** Automation reduced manual effort, increasing productivity.

**Scalability:** Designed to handle large datasets and growing business needs.

**Improved Decision-Making:** Delivered high-quality, analysis-ready data to Power BI, supporting strategic decisions.

**Industry Relevance:** Demonstrated expertise in real-world tools and architectures, adding value to data engineering teams.


## **Next Steps**

**Real-Time Processing:** Incorporate streaming data pipelines for real-time analytics.

**Cost Optimization:** Analyze and optimize Azure resource usage for cost-effectiveness.

**Enhanced Analytics:** Integrate additional visualization tools like Tableau to broaden reporting capabilities.


This project underscores my expertise in designing and implementing scalable, reliable, and impactful data engineering solutions, showcasing my readiness to contribute to enterprise-level data initiatives.
